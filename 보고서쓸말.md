# 왜 시간 차이 많이 안나?
distribute를 하면 속도가 더 훨씬 빨라질거야  
근데 만약에 원래 batch 100이고 dist 해서 batch가 50 되면 차이가 느껴질 수 있어  
하지만 우리는 gpu를 2,3,4개로 나누어서 비교할건데 일단 얘네배수 12의 배수로 batch를 설정할거야  
그래서 24해봤는데 cuda out of memory가 났어  
그래서 우리는 batch를 12로 쓸거야  
그러면 원래는 12고 dist2는 batch 6일거야  
그래서 두개 차이가 많이 안나  
그니깐 batch 크기 엄청 늘리면 잘 될고야 !!  

# 왜 halp 적용했는게 느려 ?
halp논문에는 간단한 모델 사용해서 비교만 했어  
근데 우리는 좀 복잡한 모델인 transformer 사용했다  
그래서 model parameter 를 quantize하는게 너무 큰가봐
여기서 시간이 아주 걸리나봐  
시간 비교하는거 논문에는 mnist 모델을 logistic regression한거로만 하더라  
복잡한 모델(lstm, resnet101)은 loss 만 비교하더라

# 비교할거
1. 기본모델(gpu 1개)
2. distribute(gpu 2~4)
3. HALP(8bit)+distribute(gpu 2~4)
4. HALP(4bit)+distribute(gpu 2~4)
5. HALP(16bit)+distribute(gpu 2~4)
6. SVRG+distribute(gpu 2~4)
